{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/jay.khandelwal/Documents/Personal/Python/myenv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /Users/jay.khandelwal/Documents/Personal/Python/myenv/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Users/jay.khandelwal/Documents/Personal/Python/myenv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jay.khandelwal/Documents/Personal/Python/myenv/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /Users/jay.khandelwal/Documents/Personal/Python/myenv/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, this is a sample text for demonstrating text processing using NLTK in Python.\n",
    "NLTK is a powerful library for natural language processing tasks!\n",
    "Please revise the concept learning here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jay.khandelwal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "#Sentence->Paragraph\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, this is a sample text for demonstrating text processing using NLTK in Python.\n",
      "NLTK is a powerful library for natural language processing tasks!\n",
      "Please revise the concept learning here.\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "#Paragraph->Words\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(corpus, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'demonstrating',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " 'in',\n",
       " 'Python',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'library',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " '!',\n",
       " 'Please',\n",
       " 'revise',\n",
       " 'the',\n",
       " 'concept',\n",
       " 'learning',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'this', 'is', 'a', 'sample', 'text', 'for', 'demonstrating', 'text', 'processing', 'using', 'NLTK', 'in', 'Python', '.']\n",
      "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '!']\n",
      "['Please', 'revise', 'the', 'concept', 'learning', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in documents:\n",
    "    word=word_tokenize(sentences)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer    #In between sentence fullstop doesn't split (e.g [...,'in', 'Python.', 'NLTK',...])\n",
    "from nltk.tokenize import WordPunctTokenizer      #I'm->['I', \"'\", 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'demonstrating',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " 'in',\n",
       " 'Python.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'library',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " '!',\n",
       " 'Please',\n",
       " 'revise',\n",
       " 'the',\n",
       " 'concept',\n",
       " 'learning',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program --> program\n",
      "programs --> program\n",
      "programmer --> programm\n",
      "programming --> program\n",
      "programmers --> programm\n",
      "ran --> ran\n",
      "running --> run\n",
      "runner --> runner\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "fairness --> fair\n",
      "studies --> studi\n",
      "studying --> studi\n",
      "studied --> studi\n"
     ]
    }
   ],
   "source": [
    "#Stemming is a process of reducing words to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "words=[\"program\",\"programs\",\"programmer\",\"programming\",\"programmers\",\"ran\",\"running\",\"runner\",\"easily\",\"fairly\",\"fairness\",\"studies\",\"studying\",\"studied\"]\n",
    "for w in words:\n",
    "    print(f\"{w} --> {ps.stem(w)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs=RegexpStemmer('ing$|s$|ed$|ly$|er$|ies$|ness$',min=4)\n",
    "w=\"fairly\"\n",
    "print(f\"{w} --> {rs.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program --> program\n",
      "programs --> program\n",
      "programmer --> programm\n",
      "programming --> program\n",
      "programmers --> programm\n",
      "ran --> ran\n",
      "running --> run\n",
      "runner --> runner\n",
      "easily --> easili\n",
      "fairly --> fair\n",
      "fairness --> fair\n",
      "studies --> studi\n",
      "studying --> studi\n",
      "studied --> studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer  #A little bit more advanced than PorterStemmer\n",
    "ss=SnowballStemmer('english')\n",
    "for w in words:\n",
    "    print(f\"{w} --> {ss.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This class use morphy() function internally. It is a built-in function in WordNetLemmatizer which looks for the lemma of a word in WordNet's built-in morphy function.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn=WordNetLemmatizer()\n",
    "wn.lemmatize(\"studies\",pos='n')   #pos can be 'a' for adjective, 'r' for adverb, 'v' for verb, 'n' for noun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jay.khandelwal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'below', 'there', 'was', 'be', 'so', 'y', \"i've\", 'ma', \"doesn't\", \"she's\", 'themselves', \"i'd\", 'an', 'had', \"shan't\", 'any', 'me', \"it's\", 'm', 'himself', 'in', 'own', 'its', 'them', 'who', 'her', \"mightn't\", 'o', 'do', 'whom', 'yours', \"isn't\", 'about', 'this', 's', 'it', 'mustn', 'where', \"he's\", 'he', 'herself', 'shan', \"you're\", 'after', 'are', 'because', 'under', 'for', 'myself', \"don't\", 'just', 'or', 'into', 'how', 'weren', \"it'll\", 'been', 'they', 've', 'isn', \"that'll\", 'these', 'him', \"we'd\", 'between', \"you'd\", 're', 'ours', 'against', 'off', \"didn't\", 'on', \"she'll\", 'his', 'yourselves', 'very', 'from', 'couldn', \"haven't\", 'having', 'a', 'above', 'did', 'i', \"i'll\", 'other', 'if', \"they'll\", 'further', 'some', 'each', 'but', \"mustn't\", 'will', 'why', 'don', 'wouldn', 'itself', 'needn', 'shouldn', 'theirs', 'our', \"we're\", \"needn't\", 'is', \"wouldn't\", 'you', \"you'll\", 'which', 'your', 'doing', 'haven', 'all', 'wasn', 'such', \"we've\", \"it'd\", 'mightn', \"he'd\", 'nor', 'not', \"she'd\", 'those', 'during', \"they'd\", 'd', \"they're\", 'that', 'at', 'here', 'once', 'should', 'of', 'only', \"weren't\", \"hadn't\", 'down', 'doesn', \"wasn't\", \"we'll\", 'has', \"they've\", 'as', 'same', 'up', 'too', 'll', 'have', \"won't\", 'am', 'didn', 'most', 'being', 'again', 'and', 'hadn', 't', 'the', 'does', 'over', 'to', 'when', \"you've\", 'with', 'their', 'ourselves', 'before', 'both', \"couldn't\", 'what', 'yourself', 'out', 'by', 'ain', 'than', 'hasn', 'were', 'until', 'then', 'she', 'while', 'few', \"shouldn't\", \"i'm\", \"should've\", 'won', \"aren't\", \"he'll\", 'no', 'we', 'more', 'aren', 'hers', 'my', 'can', 'now', 'through', \"hasn't\"}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"APJ Abdul Kalam was born in Rameswaram on 15 October 1931. He studied physics and aerospace engineering. He then joined DRDO and ISRO. He was also a key figure in India's Pokhran-II nuclear tests in 1998. He served as the 11th President of India from 2002 to 2007. His presidency was marked by efforts to promote education, particularly in the fields of science and technology. Kalam was known as the 'People's President' and inspired millions with his vision for a developed India. He passed away on 27 July 2015 while delivering a lecture at the Indian Institute of Management Shillong.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APJ Abdul Kalam bear Rameswaram 15 October 1931 .',\n",
       " 'He study physics aerospace engineer .',\n",
       " 'He join DRDO ISRO .',\n",
       " \"He also key figure India 's Pokhran-II nuclear test 1998 .\",\n",
       " 'He serve 11th President India 2002 2007 .',\n",
       " 'His presidency mark efforts promote education , particularly field science technology .',\n",
       " \"Kalam know 'People 's President ' inspire millions vision develop India .\",\n",
       " 'He pass away 27 July 2015 deliver lecture Indian Institute Management Shillong .']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[wn.lemmatize(word,pos='v') for word in words if word not in set(stop_words)]\n",
    "    sentences[i]=' '.join(words)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('APJ', 'NNP'), ('Abdul', 'NNP'), ('Kalam', 'NNP'), ('born', 'VBD'), ('Rameswaram', 'NNP'), ('15', 'CD'), ('October', 'NNP'), ('1931', 'CD'), ('.', '.')]\n",
      "[('He', 'PRP'), ('studied', 'VBD'), ('physics', 'NNS'), ('aerospace', 'NN'), ('engineering', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('joined', 'VBD'), ('DRDO', 'NNP'), ('ISRO', 'NNP'), ('.', '.')]\n",
      "[('He', 'PRP'), ('also', 'RB'), ('key', 'JJ'), ('figure', 'NN'), ('India', 'NNP'), (\"'s\", 'POS'), ('Pokhran-II', 'JJ'), ('nuclear', 'JJ'), ('tests', 'NNS'), ('1998', 'CD'), ('.', '.')]\n",
      "[('He', 'PRP'), ('served', 'VBD'), ('11th', 'CD'), ('President', 'NNP'), ('India', 'NNP'), ('2002', 'CD'), ('2007', 'CD'), ('.', '.')]\n",
      "[('His', 'PRP$'), ('presidency', 'NN'), ('marked', 'VBD'), ('efforts', 'NNS'), ('promote', 'JJ'), ('education', 'NN'), (',', ','), ('particularly', 'RB'), ('fields', 'NNS'), ('science', 'NN'), ('technology', 'NN'), ('.', '.')]\n",
      "[('Kalam', 'NNP'), ('known', 'VBN'), (\"'People\", 'NNP'), (\"'s\", 'POS'), ('President', 'NNP'), (\"'\", 'POS'), ('inspired', 'JJ'), ('millions', 'NNS'), ('vision', 'NN'), ('developed', 'VBD'), ('India', 'NNP'), ('.', '.')]\n",
      "[('He', 'PRP'), ('passed', 'VBD'), ('away', 'RB'), ('27', 'CD'), ('July', 'NNP'), ('2015', 'CD'), ('delivering', 'VBG'), ('lecture', 'NN'), ('Indian', 'NNP'), ('Institute', 'NNP'), ('Management', 'NNP'), ('Shillong', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/jay.khandelwal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "paragraph=\"APJ Abdul Kalam was born in Rameswaram on 15 October 1931. He studied physics and aerospace engineering. He then joined DRDO and ISRO. He was also a key figure in India's Pokhran-II nuclear tests in 1998. He served as the 11th President of India from 2002 to 2007. His presidency was marked by efforts to promote education, particularly in the fields of science and technology. Kalam was known as the 'People's President' and inspired millions with his vision for a developed India. He passed away on 27 July 2015 while delivering a lecture at the Indian Institute of Management Shillong.\"\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stop_words)]\n",
    "    pos_tagged=nltk.pos_tag(words)\n",
    "    print(pos_tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/jay.khandelwal/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jay.khandelwal/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Eiffel Tower is located in Paris. It is one of the most famous landmarks in the world. The tower was constructed in 1889 and stands at a height of 324 meters. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. The Eiffel Tower attracts millions of visitors each year and offers stunning views of the city from its observation decks.It buiilds in year 1889.\"\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "words=nltk.word_tokenize(sentence)\n",
    "pos_tagged=nltk.pos_tag(words)\n",
    "named_entity=nltk.ne_chunk(pos_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eiffel → PERSON\n",
      "Tower → ORGANIZATION\n",
      "Paris → GPE\n",
      "Gustave Eiffel → PERSON\n",
      "Eiffel Tower → ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "for chunk in named_entity:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        entity = \" \".join(c[0] for c in chunk)\n",
    "        print(entity, \"→\", chunk.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
